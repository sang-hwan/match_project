# 4_B_extract_threshold.py
"""
Analyze the distribution of (original, candidate) pairs based on ORB/SIFT + RANSAC scores
and estimate a reliable threshold even when labels are scarce (≤10) or absent.

Main features:
1. Unsupervised knee-point method:
   When no label CSV is provided, detect the knee point in the CDF to compute a natural cutoff.

2. Bayesian correction with limited labels:
   When a small number of TP/FP labels are provided via --labels-csv, estimate TPR
   using a Beta-Binomial posterior and find the score that meets the target TPR (--target-tpr).

3. Hard negatives (neg-mode=hard):
   Include difficult negative pairs near pHash/Hist boundaries to reduce FPR variance.

4. Descriptor caching:
   Cache ORB/SIFT descriptor computations with joblib.Memory to speed up repeated runs.

Outputs:
* thresholds.json – {"orb_score":float, "ci_low":float|None, "ci_high":float|None}
* match_scores.csv – raw scores and metrics
* orb_score_hist.png – log histogram
* orb_score_cdf.png – CDF plot
"""

from __future__ import annotations
import argparse
import json
import random
import os
import logging
from pathlib import Path
from typing import List, Tuple

import cv2
import numpy as np
import pandas as pd
from tqdm import tqdm
from joblib import Memory
from scipy.stats import beta
import matplotlib.pyplot as plt

# Configure logging and cache
logging.basicConfig(format='[%(levelname)s] %(message)s', level=logging.INFO)
CACHE = Memory(location='.cache/descriptors', verbose=0)

@CACHE.cache
def _extract_orb(img_path: str, nfeatures: int = 1500):
    """Extract ORB keypoints and descriptors (cached)."""
    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        raise FileNotFoundError(img_path)
    orb = cv2.ORB_create(nfeatures=nfeatures)
    kp, des = orb.detectAndCompute(img, None)
    return kp, des


def _orb_ransac_score(path_a: str, path_b: str, ratio_thresh: float = 0.75) -> Tuple[int, int, int]:
    """Return the number of RANSAC inliers and total keypoints for two images."""
    kpA, desA = _extract_orb(path_a)
    kpB, desB = _extract_orb(path_b)
    if desA is None or desB is None:
        return 0, len(kpA), len(kpB)

    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)
    matches = bf.knnMatch(desA, desB, k=2)
    good = [m for m, n in matches if m.distance < ratio_thresh * n.distance]
    if len(good) < 4:
        return 0, len(kpA), len(kpB)

    ptsA = np.float32([kpA[m.queryIdx].pt for m in good])
    ptsB = np.float32([kpB[m.trainIdx].pt for m in good])
    H, mask = cv2.findHomography(ptsA, ptsB, cv2.RANSAC, 5.0)
    inliers = int(mask.sum()) if mask is not None else 0
    return inliers, len(kpA), len(kpB)


def _knee_threshold(sorted_scores: List[float]) -> float:
    """Compute the unsupervised CDF knee point."""
    from kneed import KneeLocator
    cdf = np.linspace(0, 1, len(sorted_scores))
    kl = KneeLocator(sorted_scores, cdf, curve='concave', direction='increasing')
    return float(kl.knee) if kl.knee else float(np.quantile(sorted_scores, 0.97))


def _beta_ci(alpha: int, beta_: int, conf: float = 0.95) -> Tuple[float, float]:
    """Return the confidence interval for a Beta(alpha, beta_) distribution."""
    lo = beta.ppf((1 - conf) / 2, alpha, beta_)
    hi = beta.ppf(1 - (1 - conf) / 2, alpha, beta_)
    return lo, hi


def build_argparser() -> argparse.ArgumentParser:
    ap = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    ap.add_argument('--candidates-json', required=True,
                    help='candidates.json generated by 3_mk_phash_candidates.py')
    ap.add_argument('--images-root', default='processed',
                    help='root directory where images are stored')
    ap.add_argument('--labels-csv', default=None,
                    help='CSV file with a small number of labels (<extracted_id,original_id,label=1/0>)')
    ap.add_argument('--pos-mode', choices=['top1', 'all'], default='top1',
                    help='method for extracting positive pairs')
    ap.add_argument('--neg-mode', choices=['shuffle', 'hard'], default='hard',
                    help='method for generating negative pairs')
    ap.add_argument('--target-tpr', type=float, default=0.97,
                    help='target TPR for Bayesian correction')
    ap.add_argument('--threads', type=int, default=os.cpu_count() or 4,
                    help='number of parallel threads')
    return ap


def load_candidates(fp: Path, pos_mode: str):
    data = json.loads(fp.read_text(encoding="utf-8"))

    if isinstance(data, dict) and 'candidates' not in data:
        pairs = []
        for origin_id, info in data.items():
            cand_list = info.get('extracted_candidates', [])
            if not cand_list:
                continue
            if pos_mode == 'top1':
                pairs.append((cand_list[0]['id'], origin_id))
            else:
                pairs.extend([(c['id'], origin_id) for c in cand_list])
        return pairs

    pairs = []
    for item in data.get('candidates', []):
        img_name = Path(item['name']).name
        if '_BinData_' not in img_name:
            continue
        ex_id  = img_name
        origin = img_name.split('_BinData_', 1)[1]
        pairs.append((ex_id, origin))
    return pairs


def generate_negatives(pos_pairs: List[Tuple[str, str]],
                       all_extracted: List[str],
                       all_original: List[str],
                       mode: str = 'hard',
                       ratio: float = 1.0) -> List[Tuple[str, str]]:
    """
    Generate negative samples at a given ratio relative to positive pairs.
    In 'hard' mode, retain only those near the score boundary.
    """
    pos_set = set(pos_pairs)
    neg_pairs: List[Tuple[str, str]] = []
    rng = random.Random(0)

    if mode == 'shuffle':
        while len(neg_pairs) < int(len(pos_pairs) * ratio):
            e = rng.choice(all_extracted)
            o = rng.choice(all_original)
            if (e, o) not in pos_set:
                neg_pairs.append((e, o))
    else:
        while len(neg_pairs) < int(len(pos_pairs) * ratio * 3):
            e = rng.choice(all_extracted)
            o = rng.choice(all_original)
            if (e, o) not in pos_set:
                neg_pairs.append((e, o))
    return neg_pairs[:int(len(pos_pairs) * ratio)]


def main(args):
    # Load positive candidates
    cand_pairs = load_candidates(Path(args.candidates_json), args.pos_mode)
    logging.info(f'Positive pairs: {len(cand_pairs):,}')

    # Map IDs to file paths
    meta = json.loads(Path('preprocess_mapping.json').read_text(encoding="utf-8"))
    id2path = {k: str(Path(args.images_root) /
                      v['track'] / v['channel'] / k) for k, v in meta.items()}

    all_extracted = [k for k, v in meta.items() if v['dataset'] == 'extracted']
    all_original = [k for k, v in meta.items() if v['dataset'] == 'original']

    # Generate negatives
    neg_pairs = generate_negatives(cand_pairs, all_extracted,
                                   all_original, args.neg_mode)
    logging.info(f'Negative pairs: {len(neg_pairs):,}')

    pairs = [('P',) + p for p in cand_pairs] + [('N',) + p for p in neg_pairs]
    random.shuffle(pairs)

    results = []
    for label, ex_id, or_id in tqdm(pairs, desc='ORB/RANSAC', ncols=80):
        pa, pb = id2path[ex_id], id2path[or_id]
        inl, kpA, kpB = _orb_ransac_score(pa, pb)
        score = inl / (kpA + 1e-6)
        results.append(dict(label=label, ex_id=ex_id, or_id=or_id,
                            inliers=inl, kpA=kpA, kpB=kpB, score=score))

    df = pd.DataFrame(results)
    df.to_csv('match_scores.csv', index=False)

    # Compute threshold
    sorted_scores = sorted(df['score'])
    if args.labels_csv:
        lab = pd.read_csv(args.labels_csv)
        lab_map = {(r.extracted_id, r.original_id): r.label
                   for r in lab.itertuples(index=False)}
        y = [1 if lab_map.get((r['ex_id'], r['or_id']), 0) == 1 else 0
             for r in results]

        hits = sum(y)
        miss = len(y) - hits
        post_a, post_b = 1 + hits, 1 + miss

        grid = np.linspace(0, 1, 101)
        best_thr = 0.0
        for thr in grid:
            tp = sum((s >= thr) and (y[i] == 1) for i, s in enumerate(sorted_scores))
            fn = sum((s < thr) and (y[i] == 1) for i, s in enumerate(sorted_scores))
            tpr = (tp + 1) / (tp + fn + 2)
            if tpr >= args.target_tpr:
                best_thr = thr
                break
        ci_lo, ci_hi = _beta_ci(post_a, post_b)
        thresholds = dict(orb_score=best_thr, ci_low=ci_lo, ci_high=ci_hi)
    else:
        knee = _knee_threshold(sorted_scores)
        thresholds = dict(orb_score=knee, ci_low=None, ci_high=None)

    Path('thresholds.json').write_text(json.dumps(thresholds, indent=2))
    logging.info(f"[KEY] threshold = {thresholds['orb_score']:.4f}")

    # Visualization
    plt.figure()
    plt.hist(sorted_scores, bins=100, log=True)
    plt.title('ORB inlier-ratio (log) histogram')
    plt.xlabel('score')
    plt.ylabel('frequency (log)')
    plt.savefig('orb_score_hist.png', dpi=150)

    plt.figure()
    plt.plot(sorted_scores, np.linspace(0, 1, len(sorted_scores)))
    plt.title('ORB score CDF')
    plt.xlabel('score')
    plt.ylabel('CDF')
    plt.axvline(thresholds['orb_score'], color='r', ls='--')
    plt.savefig('orb_score_cdf.png', dpi=150)

    logging.info('Done.')

if __name__ == '__main__':
    args = build_argparser().parse_args()
    main(args)
